{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MobileNetSR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMC0eoR4ThEx"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as transforms\n",
        "from torchvision.models.vgg import vgg16\n",
        "from torchvision.utils import save_image\n",
        "import gc\n",
        "import math\n",
        "import sys, time\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtQZ-XX_hgsp",
        "outputId": "55e56c59-607b-46c8-c1d9-692b44618587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!pip install onnxruntime\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.onnx\n",
        "import onnxruntime"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.6/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (50.3.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdkyfcF81osS",
        "outputId": "e5447dbe-ae47-404e-95af-7d21c1821e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daF3ZtIYCpVw"
      },
      "source": [
        "class MobileNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MobileNet, self).__init__()\n",
        "\n",
        "        def conv_bn(inp, oup, stride):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(inp, oup, 3, stride,1, bias=False),\n",
        "                nn.PReLU()\n",
        "            )\n",
        "\n",
        "        def conv_dw(inp, oup, stride,kernel_size):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(inp, inp, kernel_size, stride,1, groups=inp, bias=False),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(inp, oup, 1, 1, bias=False),\n",
        "                nn.PReLU(),\n",
        "            )\n",
        "\n",
        "        def deconv_dw(inp, oup, upscale,kernel_size):\n",
        "          return nn.Sequential(\n",
        "              nn.Conv2d(inp, oup, 1, 1,padding=1, bias=False),\n",
        "              nn.PReLU(),\n",
        "              nn.Conv2d(oup, oup * upscale ** 2, kernel_size,padding=1,groups=oup),\n",
        "              nn.PixelShuffle(upscale),\n",
        "              nn.PReLU(), \n",
        "          )\n",
        "        \n",
        "        def res_block(inp):\n",
        "          return nn.Sequential(\n",
        "              conv_dw(inp,inp,1,3),\n",
        "              nn.PReLU(),\n",
        "              conv_dw(inp,inp,1,3),\n",
        "          )\n",
        "        \n",
        "        self.conv_1 = conv_bn(3,32,1)\n",
        "        self.res_block1 = res_block(32)\n",
        "        self.res_block2 = res_block(32)\n",
        "        self.upsample_1 = deconv_dw(32,48,2,3)\n",
        "        self.finconv = deconv_dw(48,3,1,9)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "      pre_res = self.conv_1(inp)\n",
        "      x1 = self.res_block1(pre_res)\n",
        "      x1+= pre_res\n",
        "      x2 = self.res_block2(x1)\n",
        "      x2+= x1\n",
        "      x2+= pre_res\n",
        "      x = self.upsample_1(x2)\n",
        "      x = self.finconv(x)\n",
        "      return (torch.tanh(x) + 1) / 2"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq-jWpdDkzlQ"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def conv_dw(inp, oup, stride):\n",
        "            return nn.Sequential(\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.Conv2d(inp, inp, 3, stride, groups=inp, bias=False),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Conv2d(inp, oup, 1, 1, bias=False),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.BatchNorm2d(oup)\n",
        "            )\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            conv_dw(3,64,1),\n",
        "            conv_dw(64,64,2),\n",
        "            conv_dw(64,128,1),\n",
        "            conv_dw(128,128,2),\n",
        "            conv_dw(128,256,1),\n",
        "            conv_dw(256,256,2),\n",
        "            conv_dw(256,512,1),\n",
        "            conv_dw(512,512,2),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(512,1,1,stride = 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return torch.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1O5AUb4ZVHU"
      },
      "source": [
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        vgg = vgg16(pretrained=True)\n",
        "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval().cuda()\n",
        "        for param in loss_network.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.loss_network = loss_network\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        # Adversarial Loss\n",
        "        adversarial_loss = torch.mean(1 - out_labels)\n",
        "        # Perception Loss\n",
        "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
        "        # Image Loss\n",
        "        image_loss = self.mse_loss(out_images, target_images)\n",
        "        # TV Loss\n",
        "        tv_loss = self.tv_loss(out_images)\n",
        "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
        "\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_loss_weight=1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.tv_loss_weight = tv_loss_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_size(t):\n",
        "        return t.size()[1] * t.size()[2] * t.size()[3]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3casP4z2qGxF"
      },
      "source": [
        "! cp \"/content/drive/My Drive/DIV2K_train_HR.zip\"  \"/content/\"\n",
        "! unzip DIV2K_train_HR.zip \n",
        "! rm DIV2K_train_HR.zip\n",
        "! mv /content/DIV2K_train_HR /content/dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtmPzJMc8Vrk"
      },
      "source": [
        "! mkdir /content/data/\n",
        "! mkdir /content/data/lowres/ \n",
        "! mkdir /content/data/highres/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msUGxZ7Cy4LB",
        "outputId": "1c59c790-b967-4a12-9d5d-8da5b3609862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "count = 0 \n",
        "dims = 128\n",
        "\n",
        "files = os.listdir('/content/dataset')\n",
        "percheck = 0\n",
        "\n",
        "for item in files:\n",
        "\n",
        "  filename = os.path.join('/content/dataset',item)\n",
        "  img = cv2.imread(filename)\n",
        "\n",
        "  for r in range(0,img.shape[0], dims):\n",
        "    for c in range(0,img.shape[1], dims):\n",
        "      highres_name = \"/content/data/highres/\"+ str(count) + \".png\"\n",
        "      lowres_name = \"/content/data/lowres/\"+ str(count) + \".png\"\n",
        "      cropped = img[r:r+dims, c:c+dims,:]\n",
        "      if(cropped.shape != (dims,dims,3)):\n",
        "        continue\n",
        "      cropped_lowres = cv2.resize(cropped,(int(dims/2),int(dims/2)))\n",
        "      cv2.imwrite(highres_name,cropped)\n",
        "      cv2.imwrite(lowres_name,cropped_lowres)\n",
        "      count+=1\n",
        "    \n",
        "  percheck+=1\n",
        "  print('\\r',percheck*100/len(files),end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 100.0"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk7xeQzN6a_d"
      },
      "source": [
        "!rm -rf dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwF4kZMqq3CQ"
      },
      "source": [
        "class SupaResDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    files = os.listdir(\"/content/data/lowres\")\n",
        "    self.transform = transforms.ToTensor()\n",
        "    self.toPIL = transforms.ToPILImage()\n",
        "    self.lowres_img = []\n",
        "    self.highres_img = []\n",
        "    count = 0\n",
        "    for file in files:\n",
        "      low_name = \"/content/data/lowres/\"+file\n",
        "      high_name = \"/content/data/highres/\"+file\n",
        "      self.lowres_img.append(low_name)\n",
        "      self.highres_img.append(high_name)\n",
        "\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "\n",
        "      lr_img = Image.open(self.lowres_img[i], mode='r')\n",
        "      lr_img = self.transform(lr_img)\n",
        "      hr_img = Image.open(self.highres_img[i], mode='r')\n",
        "      hr_img = self.transform(hr_img)\n",
        "\n",
        "      return lr_img,hr_img\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.lowres_img)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA_vHElGCthd"
      },
      "source": [
        "generator = MobileNet().cuda()\n",
        "discriminator = Discriminator().cuda()\n",
        "gen_loss = GeneratorLoss().cuda()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-auSz1-kChGu"
      },
      "source": [
        "batch_size = 32\n",
        "dataset = SupaResDataset()\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  dataset, \n",
        "  batch_size=batch_size, \n",
        "  shuffle=True, \n",
        "  num_workers=4,\n",
        "  pin_memory=True\n",
        ")"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz02_XTS5pVa"
      },
      "source": [
        "def save_models(generator,discriminator,model_dir,epochno):\n",
        "  torch.save(generator.state_dict(),model_dir+str(epochno)+\"_Gen.pth\")\n",
        "  torch.save(discriminator.state_dict(),model_dir+str(epochno)+\"_Dis.pth\")\n",
        "\n",
        "def test_images(generator,image_dir,epochno):\n",
        "  img = Image.open(\"/content/drive/My Drive/Datasets/Super Resolution/input.jpg\", mode='r')\n",
        "  img = Variable(transforms.ToTensor()(img), requires_grad=False).unsqueeze(0).cuda()\n",
        "  generator.eval()\n",
        "  with torch.no_grad():\n",
        "    out = generator(img)\n",
        "  generator.train()\n",
        "  out_img = transforms.ToPILImage()(out[0].data.cpu())\n",
        "  out_img.save(image_dir+str(epochno)+\".jpg\")\n",
        "\n",
        "def load_models(generator,discriminator,model_dir,num):\n",
        "  generator.load_state_dict(torch.load(model_dir+str(num)+\"_Gen.pth\"))\n",
        "  discriminator.load_state_dict(torch.load(model_dir+str(num)+\"_Dis.pth\"))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN3sOuD-62Zm"
      },
      "source": [
        "epochs = 10\n",
        "model_dir = \"/content/drive/My Drive/Datasets/Super Resolution/Model Checkpoints/\"\n",
        "img_dir = \"/content/drive/My Drive/Datasets/Super Resolution/Image checkpoints/\"\n",
        "filecount = len(os.listdir(\"/content/data/lowres\"))\n",
        "steps_per_epoch = int(filecount/batch_size)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No-YyDbgB7wP"
      },
      "source": [
        "optimizerG = torch.optim.Adam(generator.parameters())\n",
        "optimizerD = torch.optim.Adam(discriminator.parameters())"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPtIt6zLoFJf"
      },
      "source": [
        "displacement = 11"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SXj0MfwC5oL",
        "outputId": "642b1e07-757c-45f2-f22f-07b5c3b1eb5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "generator.train()\n",
        "discriminator.train()\n",
        "for i in range(epochs):\n",
        "\n",
        "  runningG_loss = 0\n",
        "  runningD_loss = 0\n",
        "  iterator = iter(train_loader)\n",
        "  print(\"Epoch \"+str(displacement+i)+\" start\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  for z in range(steps_per_epoch):\n",
        "\n",
        "    lr_img, hr_img = next(iterator)\n",
        "    lr_img = lr_img.cuda()\n",
        "    hr_img = hr_img.cuda()\n",
        "\n",
        "    generator.zero_grad()\n",
        "    fk_img = generator(lr_img)\n",
        "    fake_out = discriminator(fk_img).detach()\n",
        "    g_loss = gen_loss(fake_out,fk_img,hr_img)\n",
        "    g_loss.backward()\n",
        "    runningG_loss+=g_loss.item()\n",
        "    optimizerG.step()\n",
        "    \n",
        "    discriminator.zero_grad()\n",
        "    real_out = discriminator(hr_img).mean()\n",
        "    fake_out = discriminator(fk_img.detach()).mean()\n",
        "    d_loss = 1-real_out + fake_out\n",
        "    runningD_loss+=d_loss.item()\n",
        "    d_loss.backward()\n",
        "    optimizerD.step()    \n",
        "  \n",
        "    print(\n",
        "          '\\r',str((z+1)*100/steps_per_epoch)[0:4]+\n",
        "          \"\\t GenLoss: \"+str(runningG_loss/((z+1)*batch_size))+\n",
        "          \"\\t DiscLoss: \"+str(runningD_loss/((z+1)*batch_size))+\n",
        "          \"\\t \"+str(time.time()-start_time),\n",
        "          end=''\n",
        "         )\n",
        "  \n",
        "  save_models(generator,discriminator,model_dir,displacement+i)\n",
        "  test_images(generator,img_dir,displacement+i)\n",
        "  \n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 11 start\n",
            " 100.\t GenLoss: 7.498227774421612e-05\t DiscLoss: 2.579525898954184e-10\t 1335.5373508930206\n",
            "Epoch 12 start\n",
            " 100.\t GenLoss: 7.470391589509806e-05\t DiscLoss: 2.0462895456939256e-11\t 1332.2308876514435\n",
            "Epoch 13 start\n",
            " 100.\t GenLoss: 7.444949362647993e-05\t DiscLoss: 8.639404290613273e-12\t 1333.0729355812073\n",
            "Epoch 14 start\n",
            " 42.4\t GenLoss: 7.429587517826629e-05\t DiscLoss: 1.3132153119155893e-12\t 566.2831008434296"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkWbF02b6b50"
      },
      "source": [
        "img = Image.open(\"/content/drive/My Drive/Datasets/Super Resolution/input.jpg\", mode='r')\n",
        "img = Variable(transforms.ToTensor()(img), requires_grad=False).unsqueeze(0).cuda()\n",
        "out = generator(img)\n",
        "out_img = transforms.ToPILImage()(out[0].data.cpu())\n",
        "out_img.save(\"oup.jpg\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBEQDExroEQT"
      },
      "source": [
        "img = cv2.imread(\"oup.jpg\")\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}